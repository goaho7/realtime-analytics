
volumes:
  pg_data:
  minio_data:
    driver: local

services:

  user-api:
    build: ./user_api
    ports:
      - "8000:8000"
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped
    volumes:
      - ./user_api:/user_api
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  data-spamer:
    build: ./data_spamer
    depends_on:
      - user-api
    restart: unless-stopped
    volumes:
      - ./data_spamer:/data_spamer
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  event-processor:
    build: ./event_processor
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    command: >
      sh -c "poetry run alembic upgrade head && poetry run python main.py"
    restart: unless-stopped
    volumes:
      - ./event_processor:/event_processor
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  postgres:
    image: postgres:13
    env_file: event_processor/.env
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "sh -c 'pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}'"]
      interval: 2s
      timeout: 2s
      retries: 5
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  analytics-api:
    build: ./analytics_api
    ports:
      - "8001:8001"
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    volumes:
      - ./analytics_api:/analytics_api
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
    # environment:
    #   - CELERY_BROKER_URL=redis://redis:6379
    #   - CELERY_RESULT_BACKEND=redis://redis:6379

  kafka:
    image: confluentinc/cp-kafka:7.9.0
    hostname: broker
    container_name: broker
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker:29093'
      KAFKA_LISTENERS: 'PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'
      KAFKA_LOG_DIRS: '/tmp/kraft-combined-logs'
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "broker:29092", "--list"]
      interval: 15s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  minio:
    image: 'bitnami/minio:latest'
    ports:
      - '9000:9000'
      - '9001:9001'
    environment:
      - MINIO_ROOT_USER=user
      - MINIO_ROOT_PASSWORD=password
    depends_on:
      - analytics-api
    volumes:
      - 'minio_data:/bitnami/minio/data'

  celery-worker:
    build: ./analytics_api
    command: poetry run celery -A src.celery_core.celery_app worker --loglevel=info
    depends_on:
      - analytics-api
      - redis
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  celery-beat:
    build: ./analytics_api
    command: poetry run celery -A src.celery_core.celery_app beat --loglevel=info
    depends_on:
      - celery-worker
      - analytics-api
      - redis
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  redis:
    image: redis:alpine
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"
